{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Drought.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mskansara/multivariate_lstm/blob/master/Drought.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsfHRQX7lBsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b3db8ee-e2e7-4752-c91a-c15f1bdd833b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "NAME='DroughtPrediction'\n",
        "\n",
        "#Get dataset\n",
        "dataset = pd.read_csv('RoughSPI.csv')\n",
        "\n",
        "dataset.index = dataset['Time']\n",
        "dataset = dataset.drop(['Time'], axis=1)\n",
        "features = dataset.drop(['SPI','Location'], axis=1)\n",
        "output = dataset.drop(['Location','Rainfall','Average temp','Evapotranspiration'], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "#Removing hyphens from dates\n",
        "# features['Time'] = features['Time'].replace('-','',regex=True)\n",
        "# dataset.index.name = 'Time'\n",
        "#Assigning numbers to location\n",
        "#0-Amravati, 1-Akola, 2-Bhandara, 3-Beed, 4-Aurangabad\n",
        "# features['Location'] = features['Location'].replace('Amravati',0,regex=True)\n",
        "# features['Location'] = features['Location'].replace('Akola',1,regex=True)\n",
        "# features['Location'] = features['Location'].replace('Bhandara',2,regex=True)\n",
        "# features['Location'] = features['Location'].replace('Beed',3,regex=True)\n",
        "# features['Location'] = features['Location'].replace('Aurangabad',4,regex=True)\n",
        "\n",
        "#Normalize the data\n",
        "# features['Rainfall'] = (features['Rainfall']-min(features['Rainfall'])) / (max(features['Rainfall'])-min(features['Rainfall']))\n",
        "# features['Average temp'] = (features['Average temp']-min(features['Average temp'])) / (max(features['Average temp'])-min(features['Average temp']))\n",
        "# features['Evapotranspiration'] = (features['Evapotranspiration']-min(features['Evapotranspiration'])) / (max(features['Evapotranspiration'])-min(features['Evapotranspiration']))\n",
        "\n",
        "#Standardize the data\n",
        "feature_columns = features.columns\n",
        "output_columns = output.columns\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "features = pd.DataFrame(features,columns=feature_columns)\n",
        "\n",
        "#Removing the headers from features and output\n",
        "new_header = features.iloc[0] #grab the first row for the header\n",
        "features = features[1:] #take the data less the header row\n",
        "features.columns = new_header\n",
        "\n",
        "new_header = output.iloc[0] #grab the first row for the header\n",
        "output = output[1:] #take the data less the header row\n",
        "output.columns = new_header\n",
        "\n",
        "#Splitting features and output\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features, output, test_size=0.20)\n",
        "\n",
        "X_train = X_train.to_numpy()\n",
        "X_test = X_test.to_numpy()\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "#Create a model\n",
        "model = Sequential()\n",
        "\n",
        "#Stacked LSTM\n",
        "model.add(LSTM(128, input_shape=(1,3),activation='relu', return_sequences=True))\n",
        "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(64,activation='relu', return_sequences=True))\n",
        "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(64, activation='relu', return_sequences=False))\n",
        "# model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
        "\n",
        "optimizer = Adam(lr=1e-5, decay=1e-6)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
        "# model.summary()\n",
        "\n",
        "# fit model\n",
        "history = model.fit(X_train,Y_train,\n",
        "                    epochs=100, \n",
        "                    callbacks=[tensorboard],\n",
        "                    validation_data = (X_test,Y_test))\n",
        "\n",
        "trainPredict = model.predict(X_train)\n",
        "testPredict = model.predict(X_test)\n",
        "\n",
        "# Score model\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "\n",
        "#Calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(Y_train[0], trainPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(Y_test[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1007 samples, validate on 252 samples\n",
            "Epoch 1/100\n",
            "1007/1007 [==============================] - 15s 15ms/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 2/100\n",
            "1007/1007 [==============================] - 0s 469us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 3/100\n",
            "1007/1007 [==============================] - 0s 487us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 4/100\n",
            "1007/1007 [==============================] - 0s 469us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 5/100\n",
            "1007/1007 [==============================] - 1s 594us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 6/100\n",
            "1007/1007 [==============================] - 1s 587us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 7/100\n",
            "1007/1007 [==============================] - 1s 565us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 8/100\n",
            "1007/1007 [==============================] - 1s 511us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 9/100\n",
            "1007/1007 [==============================] - 1s 516us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 10/100\n",
            "1007/1007 [==============================] - 1s 550us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 11/100\n",
            "1007/1007 [==============================] - 1s 595us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 12/100\n",
            "1007/1007 [==============================] - 1s 498us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 13/100\n",
            "1007/1007 [==============================] - 1s 500us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 14/100\n",
            "1007/1007 [==============================] - 0s 488us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 15/100\n",
            "1007/1007 [==============================] - 1s 505us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 16/100\n",
            "1007/1007 [==============================] - 0s 493us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 17/100\n",
            "1007/1007 [==============================] - 1s 517us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 18/100\n",
            "1007/1007 [==============================] - 1s 497us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 19/100\n",
            "1007/1007 [==============================] - 1s 522us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 20/100\n",
            "1007/1007 [==============================] - 1s 508us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 21/100\n",
            "1007/1007 [==============================] - 0s 495us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 22/100\n",
            "1007/1007 [==============================] - 1s 527us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 23/100\n",
            "1007/1007 [==============================] - 1s 514us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 24/100\n",
            "1007/1007 [==============================] - 1s 523us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 25/100\n",
            "1007/1007 [==============================] - 1s 503us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 26/100\n",
            "1007/1007 [==============================] - 1s 499us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 27/100\n",
            "1007/1007 [==============================] - 1s 533us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 28/100\n",
            "1007/1007 [==============================] - 1s 640us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 29/100\n",
            "1007/1007 [==============================] - 1s 551us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 30/100\n",
            "1007/1007 [==============================] - 1s 531us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 31/100\n",
            "1007/1007 [==============================] - 1s 673us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 32/100\n",
            "1007/1007 [==============================] - 1s 560us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 33/100\n",
            "1007/1007 [==============================] - 1s 530us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 34/100\n",
            "1007/1007 [==============================] - 1s 514us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 35/100\n",
            "1007/1007 [==============================] - 1s 522us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 36/100\n",
            "1007/1007 [==============================] - 1s 521us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 37/100\n",
            "1007/1007 [==============================] - 1s 555us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 38/100\n",
            "1007/1007 [==============================] - 1s 580us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 39/100\n",
            "1007/1007 [==============================] - 1s 569us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 40/100\n",
            "1007/1007 [==============================] - 1s 568us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 41/100\n",
            "1007/1007 [==============================] - 1s 562us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 42/100\n",
            "1007/1007 [==============================] - 1s 609us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 43/100\n",
            "1007/1007 [==============================] - 1s 587us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 44/100\n",
            "1007/1007 [==============================] - 1s 581us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 45/100\n",
            "1007/1007 [==============================] - 1s 550us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 46/100\n",
            "1007/1007 [==============================] - 1s 539us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 47/100\n",
            "1007/1007 [==============================] - 1s 526us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 48/100\n",
            "1007/1007 [==============================] - 1s 556us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 49/100\n",
            "1007/1007 [==============================] - 1s 548us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 50/100\n",
            "1007/1007 [==============================] - 1s 524us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 51/100\n",
            "1007/1007 [==============================] - 1s 507us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 52/100\n",
            "1007/1007 [==============================] - 1s 516us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 53/100\n",
            "1007/1007 [==============================] - 1s 500us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 54/100\n",
            "1007/1007 [==============================] - 1s 510us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 55/100\n",
            "1007/1007 [==============================] - 1s 520us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 56/100\n",
            "1007/1007 [==============================] - 1s 537us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 57/100\n",
            "1007/1007 [==============================] - 1s 541us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 58/100\n",
            "1007/1007 [==============================] - 1s 561us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 59/100\n",
            "1007/1007 [==============================] - 1s 608us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 60/100\n",
            "1007/1007 [==============================] - 1s 578us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 61/100\n",
            "1007/1007 [==============================] - 1s 571us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 62/100\n",
            "1007/1007 [==============================] - 1s 578us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 63/100\n",
            "1007/1007 [==============================] - 1s 608us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 64/100\n",
            "1007/1007 [==============================] - 1s 579us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 65/100\n",
            "1007/1007 [==============================] - 1s 532us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 66/100\n",
            "1007/1007 [==============================] - 1s 530us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 67/100\n",
            "1007/1007 [==============================] - 1s 600us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 68/100\n",
            "1007/1007 [==============================] - 1s 617us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 69/100\n",
            "1007/1007 [==============================] - 1s 586us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 70/100\n",
            "1007/1007 [==============================] - 1s 576us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 71/100\n",
            "1007/1007 [==============================] - 1s 569us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 72/100\n",
            "1007/1007 [==============================] - 1s 569us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 73/100\n",
            "1007/1007 [==============================] - 1s 549us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 74/100\n",
            "1007/1007 [==============================] - 1s 558us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 75/100\n",
            "1007/1007 [==============================] - 1s 578us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 76/100\n",
            "1007/1007 [==============================] - 1s 566us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 77/100\n",
            "1007/1007 [==============================] - 1s 501us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 78/100\n",
            "1007/1007 [==============================] - 1s 514us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 79/100\n",
            "1007/1007 [==============================] - 1s 587us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 80/100\n",
            "1007/1007 [==============================] - 1s 571us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 81/100\n",
            "1007/1007 [==============================] - 1s 576us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 82/100\n",
            "1007/1007 [==============================] - 1s 523us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 83/100\n",
            "1007/1007 [==============================] - 1s 527us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 84/100\n",
            "1007/1007 [==============================] - 0s 482us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 85/100\n",
            "1007/1007 [==============================] - 1s 503us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 86/100\n",
            "1007/1007 [==============================] - 0s 484us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 87/100\n",
            "1007/1007 [==============================] - 1s 519us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 88/100\n",
            "1007/1007 [==============================] - 1s 527us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 89/100\n",
            "1007/1007 [==============================] - 1s 566us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 90/100\n",
            "1007/1007 [==============================] - 1s 572us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 91/100\n",
            "1007/1007 [==============================] - 1s 538us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 92/100\n",
            "1007/1007 [==============================] - 0s 494us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 93/100\n",
            "1007/1007 [==============================] - 1s 502us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 94/100\n",
            "1007/1007 [==============================] - 1s 604us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 95/100\n",
            "1007/1007 [==============================] - 1s 539us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 96/100\n",
            "1007/1007 [==============================] - 1s 503us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 97/100\n",
            "1007/1007 [==============================] - 1s 507us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 98/100\n",
            "1007/1007 [==============================] - 0s 495us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 99/100\n",
            "1007/1007 [==============================] - 1s 530us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "Epoch 100/100\n",
            "1007/1007 [==============================] - 1s 563us/step - loss: 0.8223 - acc: 0.0794 - val_loss: 0.8859 - val_acc: 0.0675\n",
            "[0.8859166417803083, 0.06746031757858065]\n",
            "Train Score: 0.91 RMSE\n",
            "Test Score: 0.94 RMSE\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}